import os
os.environ["CUDA_VISIBLE_DEVICES"] = "0,1" # <-- ì´ ì¤„ì„ ì¶”ê°€!

import pandas as pd
import torch
import numpy as np
from sklearn.metrics import accuracy_score, f1_score
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    Trainer,
    TrainingArguments,
    EarlyStoppingCallback
)
import pandas as pd
import json
import glob # ì—¬ëŸ¬ íŒŒì¼ ê²½ë¡œë¥¼ í•œ ë²ˆì— ê°€ì ¸ì˜¤ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬

# --- 1. íŒŒì¼ ê²½ë¡œ ì„¤ì • ---
# ì‚¬ìš©í•˜ì‹œëŠ” í´ë” ê²½ë¡œì— ë§ê²Œ ìˆ˜ì •í•˜ì„¸ìš”.
train_json_files = glob.glob(r'./02/*.json')
validate_json_files = glob.glob(r'./Validation/02/*.json')

# --- 2. ì—¬ëŸ¬ JSON íŒŒì¼ì„ ì½ì–´ í•˜ë‚˜ì˜ DataFrameìœ¼ë¡œ ë§Œë“œëŠ” í•¨ìˆ˜ ---
def create_dataframe_from_json(file_paths):
    """
    ì£¼ì–´ì§„ ê²½ë¡œì˜ ëª¨ë“  JSON íŒŒì¼ì„ ì½ì–´,
    [ëŒ€í™”ë¬¸, ë¼ë²¨] í˜•íƒœì˜ Pandas DataFrameìœ¼ë¡œ ë§Œë“¤ì–´ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    training_data = []
    for file_path in file_paths:
        with open(file_path, 'r', encoding='utf-8') as f:
            conversation = json.load(f)
        
        utterances = conversation['utterances']
        # ì´ì „ ì½”ë“œì™€ ë™ì¼í•œ ë¡œì§ìœ¼ë¡œ í™”ì/ì²­ì ëŒ€í™”ë¥¼ ì§ì§€ì–´ ì¤ë‹ˆë‹¤.
        for i in range(len(utterances) - 1):
            if utterances[i]['role'] == 'speaker' and utterances[i+1]['role'] == 'listener':
                speaker_text = utterances[i]['text']
                # listener_empathyê°€ ìˆìœ¼ë©´ í•´ë‹¹ ë¼ë²¨ì„, ì—†ìœ¼ë©´ 'ì¤‘ë¦½' ë¼ë²¨ì„ ë¶€ì—¬
                if utterances[i+1]['listener_empathy']:
                    label = utterances[i+1]['listener_empathy'][0]
                else:
                    label = 'ì¤‘ë¦½' # ìƒˆë¡œìš´ ë¼ë²¨ ë¶€ì—¬

                training_data.append([speaker_text, label])
                    
    return pd.DataFrame(training_data, columns=['text', 'label'])

# --- 3. í•¨ìˆ˜ë¥¼ ì´ìš©í•´ train / validate ë°ì´í„°í”„ë ˆì„ ìƒì„± ---
train_df = create_dataframe_from_json(train_json_files)
validate_df = create_dataframe_from_json(validate_json_files)

print(f"í›ˆë ¨ ë°ì´í„° ê°œìˆ˜: {len(train_df)}")
print(f"ê²€ì¦ ë°ì´í„° ê°œìˆ˜: {len(validate_df)}")
print("\n--- í›ˆë ¨ ë°ì´í„° ìƒ˜í”Œ ---")
print(train_df.head())
print("\n--- ê²€ì¦ ë°ì´í„° ìƒ˜í”Œ ---")
print(validate_df.head())
# [3ë‹¨ê³„] ... train_df, validate_df ìƒì„± í›„
print("\n--- í›ˆë ¨ ë°ì´í„° ë¼ë²¨ ë¶„í¬ í™•ì¸ ---")
print(validate_df['label'].value_counts())

# --- 4. ë¼ë²¨ì„ ìˆ«ìë¡œ ë³€í™˜ (Label Encoding) ---
# í›ˆë ¨ ë°ì´í„°ì— ìˆëŠ” ëª¨ë“  ë¼ë²¨ì„ ê¸°ì¤€ìœ¼ë¡œ ì‚¬ì „ì„ ë§Œë“­ë‹ˆë‹¤.
labels = train_df['label'].unique().tolist()
label_to_id = {label: i for i, label in enumerate(labels)}
id_to_label = {i: label for i, label in enumerate(labels)}

# train_dfì™€ validate_df ëª¨ë‘ì— ë™ì¼í•œ ê·œì¹™ì„ ì ìš©í•©ë‹ˆë‹¤.
train_df['label_id'] = train_df['label'].map(label_to_id)
validate_df['label_id'] = validate_df['label'].map(label_to_id)
# =================================================================
# 1ë‹¨ê³„: ë°ì´í„° ë¡œë“œ
# =================================================================

num_labels = len(labels)
print(f"ì´ ë¼ë²¨ ê°œìˆ˜: {num_labels}")
print(f"ë¼ë²¨ ì¢…ë¥˜: {labels}")
print(f"ë¼ë²¨ -> ID ë§µí•‘: {label_to_id}")


# =================================================================
# 3ë‹¨ê³„: ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì¤€ë¹„
# =================================================================
print("\nâœ… 3ë‹¨ê³„: ëª¨ë¸ ë° í† í¬ë‚˜ì´ì €ë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤.")
#MODEL_NAME = "monologg/koelectra-base-v3-discriminator"
MODEL_NAME = "klue/roberta-large"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)


# =================================================================
# 4ë‹¨ê³„: ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ë° í† í°í™”
# =================================================================
print("\nâœ… 4ë‹¨ê³„: íŒŒì´í† ì¹˜ ë°ì´í„°ì…‹ì„ ìƒì„±í•©ë‹ˆë‹¤.")
# í…ìŠ¤íŠ¸ë¥¼ í† í°í™”
train_encodings = tokenizer(list(train_df['text']), truncation=True, padding=True, max_length=128)
val_encodings = tokenizer(list(validate_df['text']), truncation=True, padding=True, max_length=128)

# íŒŒì´í† ì¹˜ ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜
class EmpathyDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

# ë°ì´í„°ì…‹ ê°ì²´ ìƒì„±
train_dataset = EmpathyDataset(train_encodings, list(train_df['label_id']))
val_dataset = EmpathyDataset(val_encodings, list(validate_df['label_id']))


# =================================================================
# 5ë‹¨ê³„: ì„±ëŠ¥ ì§€í‘œ ê³„ì‚° í•¨ìˆ˜ ì •ì˜
# =================================================================
print("\nâœ… 5ë‹¨ê³„: ì„±ëŠ¥ ì§€í‘œ ê³„ì‚° í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.")
def compute_metrics(p):
    pred, labels = p
    pred = np.argmax(pred, axis=1)
    
    accuracy = accuracy_score(y_true=labels, y_pred=pred)
    f1 = f1_score(y_true=labels, y_pred=pred, average='weighted')
    
    return {"accuracy": accuracy, "f1": f1}


# =================================================================
# 6ë‹¨ê³„: í›ˆë ¨ ì¸ì ì„¤ì • ë° Trainer ìƒì„±
# =================================================================
print("\nâœ… 6ë‹¨ê³„: í›ˆë ¨ ì¸ìë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.")
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3, # í›ˆë ¨ ì—í­ ìˆ˜ (ì „ì²´ ë°ì´í„°ë¥¼ ëª‡ ë²ˆ í•™ìŠµí• ì§€)
    per_device_train_batch_size=32, # í•œ ë²ˆì— ì²˜ë¦¬í•  í›ˆë ¨ ë°ì´í„° ì–‘
    per_device_eval_batch_size=32, # í•œ ë²ˆì— ì²˜ë¦¬í•  ê²€ì¦ ë°ì´í„° ì–‘
    logging_dir='./logs',
    logging_steps=50,
    evaluation_strategy="epoch", # ë§¤ ì—í­ë§ˆë‹¤ ì„±ëŠ¥ ì¸¡ì •
    save_strategy="epoch",
    learning_rate=1e-5,       # ë§¤ ì—í­ë§ˆë‹¤ ëª¨ë¸ ì €ì¥
    load_best_model_at_end=True, # í›ˆë ¨ ì¢…ë£Œ í›„ ê°€ì¥ ì¢‹ì€ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜´
    metric_for_best_model="f1",  # ìµœê³  ëª¨ë¸ ì„ ì • ê¸°ì¤€
    save_total_limit=1,
    weight_decay=0.01,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics,
    tokenizer=tokenizer,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)] # <-- ì¡°ê¸° ì¢…ë£Œ ì½œë°± ì¶”
)


# =================================================================
# 7ë‹¨ê³„: ëª¨ë¸ í›ˆë ¨
# =================================================================
print("\nâœ… 7ë‹¨ê³„: ëª¨ë¸ í›ˆë ¨ì„ ì‹œì‘í•©ë‹ˆë‹¤. (ì‹œê°„ì´ ë‹¤ì†Œ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
trainer.train()


# =================================================================
# 8ë‹¨ê³„: ëª¨ë¸ ìµœì¢… í‰ê°€
# =================================================================
print("\nâœ… 8ë‹¨ê³„: í›ˆë ¨ëœ ëª¨ë¸ì˜ ìµœì¢… ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.")
final_metrics = trainer.evaluate()
print("--- ìµœì¢… ì„±ëŠ¥ í‰ê°€ ê²°ê³¼ ---")
for key, value in final_metrics.items():
    print(f"{key}: {value:.4f}")


# =================================================================
# 9ë‹¨ê³„: í›ˆë ¨ëœ ëª¨ë¸ë¡œ ì˜ˆì¸¡í•˜ê¸°
# =================================================================
print("\nâœ… 9ë‹¨ê³„: ìƒˆë¡œìš´ ë¬¸ì¥ìœ¼ë¡œ ì˜ˆì¸¡ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.")
# ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì•˜ë˜ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
best_model_path = trainer.state.best_model_checkpoint
best_model = AutoModelForSequenceClassification.from_pretrained(best_model_path)

def predict_empathy(text):
    """
    ìƒˆë¡œìš´ ë¬¸ì¥ì„ ì…ë ¥ë°›ì•„ í•„ìš”í•œ ê³µê° ë°˜ì‘ì„ ì˜ˆì¸¡í•˜ëŠ” í•¨ìˆ˜
    """
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
    
    # GPU ì‚¬ìš©ì´ ê°€ëŠ¥í•˜ë‹¤ë©´ ì…ë ¥ì„ GPUë¡œ ë³´ëƒ„
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    inputs = {k: v.to(device) for k, v in inputs.items()}
    best_model.to(device) # ëª¨ë¸ë„ ë™ì¼í•œ ì¥ì¹˜ë¡œ ì´ë™
    
    with torch.no_grad():
        outputs = best_model(**inputs)
        
    predicted_class_id = outputs.logits.argmax().item()
    return id_to_label[predicted_class_id]

# í…ŒìŠ¤íŠ¸ ë¬¸ì¥
text1 = "ì˜¤ëŠ˜ ì§ì¥ ìƒì‚¬í•œí…Œ ë„ˆë¬´ ì‹¬í•˜ê²Œ í˜¼ë‚˜ì„œ ê¸°ìš´ì´ í•˜ë‚˜ë„ ì—†ì–´."
text2 = "ë‚˜ ë‹¤ìŒ ì£¼ë¶€í„° ìƒˆë¡œìš´ ìš´ë™ ì‹œì‘í•´ë³¼ê¹Œ í•˜ëŠ”ë°, ë­˜ í•˜ë©´ ì¢‹ì„ê¹Œ?"
text3 = "ë„¤ ë•ë¶„ì— í”„ë¡œì íŠ¸ ì˜ ë§ˆì³¤ì–´. ì •ë§ ê³ ë§ˆì›Œ!"

print(f"ì…ë ¥ ë¬¸ì¥: '{text1}'")
print(f"ì˜ˆì¸¡ ê²°ê³¼: '{predict_empathy(text1)}'\n")

print(f"ì…ë ¥ ë¬¸ì¥: '{text2}'")
print(f"ì˜ˆì¸¡ ê²°ê³¼: '{predict_empathy(text2)}'\n")

print(f"ì…ë ¥ ë¬¸ì¥: '{text3}'")
print(f"ì˜ˆì¸¡ ê²°ê³¼: '{predict_empathy(text3)}'\n")

print("ğŸ‰ ëª¨ë“  ê³¼ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
print("\nâœ… 10ë‹¨ê³„: ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì„ ë³„ë„ í´ë”ì— ì €ì¥í•©ë‹ˆë‹¤.")

# "my_best_model" ì´ë¼ëŠ” ì´ë¦„ì˜ í´ë”ì— ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì„ ì €ì¥í•©ë‹ˆë‹¤.
trainer.save_model("my_best_5_model")
