from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import os
from typing import List
import json

app = FastAPI(
    title="Text Empathy Classification API",
    description="í…ìŠ¤íŠ¸ ê³µê° ìœ í˜•ì„ ë¶„ë¥˜í•˜ëŠ” API",
    version="1.0.0"
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ëª¨ë¸ ê²½ë¡œ ì„¤ì •
script_directory = os.path.dirname(os.path.abspath(__file__))
MODEL_PATH = os.path.join(script_directory, "my_best_model")

# ë¼ë²¨ ì •ë³´
id_to_label = {
    0: 'ê²©ë ¤',
    1: 'ìœ„ë¡œ',
    2: 'ë™ì¡°',
    3: 'ì¡°ì–¸'
}

# ì±„íŒ…ë°© ê´€ë¦¬ë¥¼ ìœ„í•œ í´ë˜ìŠ¤
class ConnectionManager:
    def __init__(self):
        self.active_connections: List[WebSocket] = []
        
    async def connect(self, websocket: WebSocket):
        await websocket.accept()
        self.active_connections.append(websocket)
    
    def disconnect(self, websocket: WebSocket):
        self.active_connections.remove(websocket)
    
    async def broadcast(self, message: dict):
        for connection in self.active_connections:
            await connection.send_json(message)

manager = ConnectionManager()

# ëª¨ë¸ í´ë˜ìŠ¤ë“¤
class TextRequest(BaseModel):
    text: str

class PredictionResponse(BaseModel):
    text: str
    prediction: str

# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
try:
    tokenizer = AutoTokenizer.from_pretrained("seunghyunq/text-classification-model")
    model = AutoModelForSequenceClassification.from_pretrained("seunghyunq/text-classification-model")
    model.eval()
    print("âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
except Exception as e:
    print(f"ğŸš¨ ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    raise Exception("ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")

def get_prediction(text: str) -> str:
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
    with torch.no_grad():
        outputs = model(**inputs)
    predicted_class_id = outputs.logits.argmax().item()
    return id_to_label.get(predicted_class_id, "ì•Œ ìˆ˜ ì—†ëŠ” ë¼ë²¨")

@app.websocket("/ws/chat")
async def websocket_endpoint(websocket: WebSocket):
    await manager.connect(websocket)
    try:
        while True:
            data = await websocket.receive_text()
            try:
                message_data = json.loads(data)
                text = message_data.get('text', '')
                
                # ë©”ì‹œì§€ ë¶„ë¥˜ ìˆ˜í–‰
                prediction = get_prediction(text)
                
                # ì‘ë‹µ ë©”ì‹œì§€ êµ¬ì„±
                response_message = {
                    "text": text,
                    "prediction": prediction
                }
                
                # ëª¨ë“  ì—°ê²°ëœ í´ë¼ì´ì–¸íŠ¸ì—ê²Œ ë¸Œë¡œë“œìºìŠ¤íŠ¸
                await manager.broadcast(response_message)
                
            except json.JSONDecodeError:
                await websocket.send_json({
                    "error": "Invalid JSON format"
                })
    except WebSocketDisconnect:
        manager.disconnect(websocket)

@app.get("/")
async def root():
    return {"message": "Text Empathy Classification API is running"} 